{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshaelon/AI_Agents/blob/main/MCQ_generation_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vUEDJZAsOG9sAFhGiaCl42kGNMZkN6gm?usp=sharing)"
      ],
      "metadata": {
        "id": "0762mPCZMSXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Agents\n",
        "\n",
        "AI Agents to create questions using GPT-4o"
      ],
      "metadata": {
        "id": "PGDL5sbBHaWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU crewai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2D6enyWyfmk",
        "outputId": "5a78acf1-1410-4d60-8dec-d7271f8db33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.2/210.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.6/327.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.2/396.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = ''"
      ],
      "metadata": {
        "id": "1BZWOQ6-zQf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt3 = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
        "gpt4 = ChatOpenAI(model = \"gpt-4o\")"
      ],
      "metadata": {
        "id": "Pyi7AJcIGrR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "\n",
        "question_creator = Agent(\n",
        "role='Data Science Expert',\n",
        "goal='Create challenging and thought-provoking questions for Data Science students',\n",
        "backstory=\"\"\"You are an experienced educator specializing in Data Science, with a focus on creating high-quality questions for assessing the analytical skills, problem-solving abilities, and conceptual understanding of students. Your goal is to develop 10 questions that test the students' knowledge in Data Science, similar to the rigorous standards of the IIT-JEE examination in India.\n",
        "\n",
        "Please ensure that the questions cover a range of topics within Data Science, including but not limited to statistics, machine learning, data analysis, and programming. The questions should be challenging, encourage deep thinking, and be designed to evaluate the students' understanding of core concepts and their ability to apply these concepts to solve problems.\"\"\",\n",
        "verbose=True,\n",
        "allow_delegation=False,\n",
        "llm = gpt4\n",
        ")\n",
        "\n",
        "question_reviewer = Agent(\n",
        "role=' Question Reviewer',\n",
        "goal='Review and refine IIT-JEE questions to ensure quality and relevance',\n",
        "backstory=\"\"\"You are an expert reviewer of  questions with a keen eye for detail and a deep understanding of the IT industry needs.\n",
        "Your role is to review the questions created by the Question Creator and provide constructive feedback to improve their quality and relevance.\n",
        "You ensure that the questions are well-structured, unambiguous, and align with the IT industry standards and difficulty level.\"\"\",\n",
        "verbose=True,\n",
        "allow_delegation=True,\n",
        "llm = gpt4\n",
        ")\n",
        "\n",
        "# Create tasks for your agents\n",
        "\n",
        "desc1 = \"\"\"You are an experienced educator specializing in Data Science, focusing on creating high-quality questions that test the analytical skills, problem-solving abilities, and conceptual understanding of students. Your goal is to develop a challenging and thought-provoking question for students in the Data Science domain.\n",
        "\n",
        "The question should cover a specific topic within Data Science, such as data cleaning, feature engineering, or model development. The question should be designed to assess the student's ability to apply core concepts to real-world scenarios, encouraging deep analytical thinking.\n",
        "\n",
        "Please provide the following:\n",
        "\n",
        "Question Statement: Clearly outline the problem, including any necessary context or data that students will need to analyze.\n",
        "Correct Answer: Offer a detailed explanation of the solution, including the steps or reasoning that lead to the correct answer.\n",
        "The final question and answer should be formatted neatly in a text document, suitable for use in an academic setting.\"\"\"\n",
        "\n",
        "task1 = Task(\n",
        "description=desc1,\n",
        "expected_output=\"A well-crafted set of 10 questions along with answers stored in a text file in the local location\",\n",
        "agent=question_creator\n",
        ")\n",
        "task2 = Task(\n",
        "description=\"\"\"Review the questions generated by the question_creator agent.\n",
        "Assess the question's quality, relevance to the IT industry standards, difficulty level, and clarity.\n",
        "Provide constructive feedback and suggestions for improvement, if necessary.\n",
        "Ensure that the question is well-structured, unambiguous, and aligns with the IT Industry standards.\"\"\",\n",
        "expected_output=\"A comprehensive review of the question with topics covered stored in a text file named - question_reviews\",\n",
        "agent=question_reviewer\n",
        ")\n",
        "\n",
        "# Instantiate your crew with a sequential process\n",
        "crew = Crew(\n",
        "agents=[question_creator, question_reviewer],\n",
        "tasks=[task1, task2],\n",
        "verbose=1,\n",
        ")\n",
        "\n",
        "# Get your crew to work!\n",
        "result = crew.kickoff()\n",
        "print(\"######################\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrX_VbLkFmbR",
        "outputId": "539dac7c-0c77-4c83-c457-9fcda962e795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m [2024-09-01 08:14:26][DEBUG]: == Working Agent: Data Science Expert\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-01 08:14:26][INFO]: == Starting Task: You are an experienced educator specializing in Data Science, focusing on creating high-quality questions that test the analytical skills, problem-solving abilities, and conceptual understanding of students. Your goal is to develop a challenging and thought-provoking question for students in the Data Science domain.\n",
            "\n",
            "The question should cover a specific topic within Data Science, such as data cleaning, feature engineering, or model development. The question should be designed to assess the student's ability to apply core concepts to real-world scenarios, encouraging deep analytical thinking.\n",
            "\n",
            "Please provide the following:\n",
            "\n",
            "Question Statement: Clearly outline the problem, including any necessary context or data that students will need to analyze.\n",
            "Correct Answer: Offer a detailed explanation of the solution, including the steps or reasoning that lead to the correct answer.\n",
            "The final question and answer should be formatted neatly in a text document, suitable for use in an academic setting.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI now can give a great answer.\n",
            "\n",
            "Final Answer: my best complete final answer to the task.\n",
            "\n",
            "---\n",
            "\n",
            "### Question 1: Statistics and Probability\n",
            "**Question Statement:**\n",
            "Given a dataset containing the heights (in cm) of 1000 students, you are required to calculate the 95% confidence interval for the mean height. Assume the heights follow a normal distribution with a sample mean of 165 cm and a standard deviation of 10 cm.\n",
            "\n",
            "**Correct Answer:**\n",
            "To calculate the 95% confidence interval for the mean height:\n",
            "1. Identify the sample mean (\\(\\bar{x}\\)) = 165 cm.\n",
            "2. Identify the standard deviation (\\(\\sigma\\)) = 10 cm.\n",
            "3. Determine the z-value for a 95% confidence interval, which is 1.96.\n",
            "4. Use the formula for the confidence interval: \\(\\bar{x} \\pm z \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\).\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 1.96 \\left(\\frac{10}{\\sqrt{1000}}\\right)\n",
            "   \\]\n",
            "\n",
            "5. Calculate the margin of error:\n",
            "\n",
            "   \\[\n",
            "   1.96 \\left(\\frac{10}{31.62}\\right) \\approx 1.96 \\times 0.3162 \\approx 0.62\n",
            "   \\]\n",
            "\n",
            "6. The confidence interval is:\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 0.62 \\rightarrow [164.38, 165.62]\n",
            "   \\]\n",
            "\n",
            "### Question 2: Machine Learning\n",
            "**Question Statement:**\n",
            "Explain the bias-variance tradeoff and how it impacts the performance of machine learning models. Provide a real-world example to illustrate your explanation.\n",
            "\n",
            "**Correct Answer:**\n",
            "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between the error due to bias and the error due to variance. \n",
            "\n",
            "- **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).\n",
            "- **Variance** refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).\n",
            "\n",
            "**Real-world Example:**\n",
            "Consider a model predicting house prices. A linear regression model may have high bias because it cannot capture non-linear relationships in the data, resulting in underfitting. On the other hand, a complex model like a high-degree polynomial regression may have high variance, capturing noise and leading to overfitting. The goal is to find a balance where both bias and variance are minimized, often through techniques like cross-validation, regularization, and choosing appropriate model complexity.\n",
            "\n",
            "### Question 3: Data Cleaning\n",
            "**Question Statement:**\n",
            "You are given a dataset with missing values in several columns. Describe at least three techniques you can use to handle missing data and discuss the pros and cons of each method.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Removing Rows with Missing Values:**\n",
            "   - **Pros:** Simple to implement and ensures no missing data remains.\n",
            "   - **Cons:** Can result in significant loss of data, especially if missing values are prevalent.\n",
            "\n",
            "2. **Imputation with Mean/Median/Mode:**\n",
            "   - **Pros:** Preserves data size, simple to implement, and works well with numerical data.\n",
            "   - **Cons:** Can introduce bias, particularly if the missing data is not randomly distributed.\n",
            "\n",
            "3. **Using Algorithms that Support Missing Values:**\n",
            "   - **Pros:** Some machine learning algorithms (e.g., decision trees) can handle missing values natively, avoiding the need for imputation.\n",
            "   - **Cons:** Limited to specific algorithms and may not be applicable for all types of data.\n",
            "\n",
            "### Question 4: Feature Engineering\n",
            "**Question Statement:**\n",
            "You have a dataset containing the timestamp of transactions. Explain how you can create new features from the timestamp to improve the predictive power of your model.\n",
            "\n",
            "**Correct Answer:**\n",
            "From the timestamp, you can extract several useful features:\n",
            "1. **Day of the week:** Indicates if the transaction was made on a weekday or weekend, which can capture weekly patterns.\n",
            "2. **Hour of the day:** Helps in understanding the time-of-day effect on transactions.\n",
            "3. **Month of the year:** Captures seasonal effects.\n",
            "4. **Holiday indicator:** Indicates whether the transaction occurred on a holiday, which can affect transaction volume.\n",
            "5. **Elapsed time since previous transaction:** Provides temporal spacing between transactions, useful for time-series analysis.\n",
            "\n",
            "### Question 5: Model Development\n",
            "**Question Statement:**\n",
            "Describe the steps involved in developing a machine learning model from scratch, starting from data collection to model deployment.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Collection:** Gather data relevant to the problem.\n",
            "2. **Data Cleaning:** Handle missing values, remove duplicates, and correct errors.\n",
            "3. **Exploratory Data Analysis (EDA):** Understand the data distribution, relationships, and patterns.\n",
            "4. **Feature Engineering:** Create new features, normalize/scale data, and select important features.\n",
            "5. **Model Selection:** Choose appropriate algorithms based on the problem type and data characteristics.\n",
            "6. **Model Training:** Split data into training and validation sets, and train the model.\n",
            "7. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, F1-score, etc.\n",
            "8. **Hyperparameter Tuning:** Optimize model performance through techniques like grid search or random search.\n",
            "9. **Model Validation:** Validate the model on a separate test set to ensure it generalizes well.\n",
            "10. **Model Deployment:** Deploy the model in a production environment and monitor its performance.\n",
            "\n",
            "### Question 6: Data Analysis\n",
            "**Question Statement:**\n",
            "You have a dataset containing customer reviews for a product. Explain how you would perform sentiment analysis on this dataset and the steps involved.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Preprocessing:** Clean the text data by removing noise (e.g., HTML tags, special characters), converting text to lowercase, and tokenizing.\n",
            "2. **Text Normalization:** Apply stemming or lemmatization to reduce words to their base forms.\n",
            "3. **Feature Extraction:** Convert text to numerical features using techniques like TF-IDF, word embeddings, or bag-of-words.\n",
            "4. **Model Selection:** Choose a sentiment analysis model (e.g., Naive Bayes, LSTM, Transformer-based models).\n",
            "5. **Model Training:** Train the model using labeled sentiment data (positive/negative/neutral).\n",
            "6. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, and F1-score.\n",
            "7. **Prediction:** Apply the trained model to predict the sentiment of customer reviews in the dataset.\n",
            "\n",
            "### Question 7: Programming\n",
            "**Question Statement:**\n",
            "Write a Python function to calculate the F1-score given the precision and recall values. Explain the significance of the F1-score in model evaluation.\n",
            "\n",
            "**Correct Answer:**\n",
            "```python\n",
            "def f1_score(precision, recall):\n",
            "    if precision + recall == 0:\n",
            "        return 0\n",
            "    return 2 * (precision * recall) / (precision + recall)\n",
            "\n",
            "# Example usage:\n",
            "precision = 0.75\n",
            "recall = 0.80\n",
            "print(f1_score(precision, recall))  # Output: 0.774\n",
            "```\n",
            "\n",
            "**Significance:**\n",
            "The F1-score is the harmonic mean of precision and recall. It is significant because it provides a single metric that balances both precision and recall, especially useful in cases where there is an uneven class distribution or when both false positives and false negatives need to be minimized.\n",
            "\n",
            "### Question 8: Time Series Analysis\n",
            "**Question Statement:**\n",
            "Explain the concept of stationarity in time series analysis. How can you test for stationarity, and why is it important for time series forecasting?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Concept:** A time series is stationary if its statistical properties (mean, variance, autocorrelation) are constant over time. Stationarity is crucial because many time series forecasting methods, like ARIMA, assume the series is stationary.\n",
            "\n",
            "**Testing for Stationarity:**\n",
            "1. **Visual Inspection:** Plot the time series and look for trends, seasonality, and constant mean/variance.\n",
            "2. **Statistical Tests:** Perform tests like the Augmented Dickey-Fuller (ADF) test or the KPSS test.\n",
            "\n",
            "**Importance:** Stationarity ensures that the model parameters remain consistent over time, leading to more reliable and accurate forecasts. Non-stationary data can result in misleading model parameters and poor forecasting performance.\n",
            "\n",
            "### Question 9: Dimensionality Reduction\n",
            "**Question Statement:**\n",
            "Describe Principal Component Analysis (PCA) and its application in reducing the dimensionality of a dataset. What are the potential benefits and drawbacks of using PCA?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Description:** PCA is a dimensionality reduction technique that transforms a dataset into a set of orthogonal components, ordered by the amount of variance they explain in the data. The first few principal components capture the most significant variance, allowing for reduction in dimensionality while retaining most of the information.\n",
            "\n",
            "**Application:**\n",
            "1. Standardize the data.\n",
            "2. Compute the covariance matrix.\n",
            "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
            "4. Sort the eigenvectors by eigenvalues in descending order.\n",
            "5. Select the top k eigenvectors to form the new feature space.\n",
            "\n",
            "**Benefits:**\n",
            "- Reduces computational complexity.\n",
            "- Mitigates the curse of dimensionality.\n",
            "- Removes noise and redundant features.\n",
            "\n",
            "**Drawbacks:**\n",
            "- PCA is sensitive to scaling.\n",
            "- It may lose interpretability of the transformed features.\n",
            "- Not suitable for non-linear relationships.\n",
            "\n",
            "### Question 10: Model Evaluation\n",
            "**Question Statement:**\n",
            "You have built a classification model and obtained the following confusion matrix:\n",
            "\n",
            "|            | Predicted Positive | Predicted Negative |\n",
            "|------------|---------------------|---------------------|\n",
            "| Actual Positive | 50                  | 10                  |\n",
            "| Actual Negative | 5                   | 35                  |\n",
            "\n",
            "Calculate the following metrics: accuracy, precision, recall, and F1-score.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Accuracy:**\n",
            "   \\[\n",
            "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{50 + 35}{50 + 35 + 5 + 10} = \\frac{85}{100} = 0.85\n",
            "   \\]\n",
            "\n",
            "2. **Precision:**\n",
            "   \\[\n",
            "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{50}{50 + 5} = \\frac{50}{55} \\approx 0.909\n",
            "   \\]\n",
            "\n",
            "3. **Recall:**\n",
            "   \\[\n",
            "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{50}{50 + 10} = \\frac{50}{60} \\approx 0.833\n",
            "   \\]\n",
            "\n",
            "4. **F1-score:**\n",
            "   \\[\n",
            "   \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.909 \\times 0.833}{0.909 + 0.833} \\approx 0.87\n",
            "   \\]\n",
            "\n",
            "---\n",
            "\n",
            "These questions and answers assess a wide range of skills and concepts in data science, including statistical analysis, machine learning, data cleaning, feature engineering, and programming, ensuring a comprehensive evaluation of the students' understanding and abilities.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-01 08:14:46][DEBUG]: == [Data Science Expert] Task output: my best complete final answer to the task.\n",
            "\n",
            "---\n",
            "\n",
            "### Question 1: Statistics and Probability\n",
            "**Question Statement:**\n",
            "Given a dataset containing the heights (in cm) of 1000 students, you are required to calculate the 95% confidence interval for the mean height. Assume the heights follow a normal distribution with a sample mean of 165 cm and a standard deviation of 10 cm.\n",
            "\n",
            "**Correct Answer:**\n",
            "To calculate the 95% confidence interval for the mean height:\n",
            "1. Identify the sample mean (\\(\\bar{x}\\)) = 165 cm.\n",
            "2. Identify the standard deviation (\\(\\sigma\\)) = 10 cm.\n",
            "3. Determine the z-value for a 95% confidence interval, which is 1.96.\n",
            "4. Use the formula for the confidence interval: \\(\\bar{x} \\pm z \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\).\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 1.96 \\left(\\frac{10}{\\sqrt{1000}}\\right)\n",
            "   \\]\n",
            "\n",
            "5. Calculate the margin of error:\n",
            "\n",
            "   \\[\n",
            "   1.96 \\left(\\frac{10}{31.62}\\right) \\approx 1.96 \\times 0.3162 \\approx 0.62\n",
            "   \\]\n",
            "\n",
            "6. The confidence interval is:\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 0.62 \\rightarrow [164.38, 165.62]\n",
            "   \\]\n",
            "\n",
            "### Question 2: Machine Learning\n",
            "**Question Statement:**\n",
            "Explain the bias-variance tradeoff and how it impacts the performance of machine learning models. Provide a real-world example to illustrate your explanation.\n",
            "\n",
            "**Correct Answer:**\n",
            "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between the error due to bias and the error due to variance. \n",
            "\n",
            "- **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).\n",
            "- **Variance** refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).\n",
            "\n",
            "**Real-world Example:**\n",
            "Consider a model predicting house prices. A linear regression model may have high bias because it cannot capture non-linear relationships in the data, resulting in underfitting. On the other hand, a complex model like a high-degree polynomial regression may have high variance, capturing noise and leading to overfitting. The goal is to find a balance where both bias and variance are minimized, often through techniques like cross-validation, regularization, and choosing appropriate model complexity.\n",
            "\n",
            "### Question 3: Data Cleaning\n",
            "**Question Statement:**\n",
            "You are given a dataset with missing values in several columns. Describe at least three techniques you can use to handle missing data and discuss the pros and cons of each method.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Removing Rows with Missing Values:**\n",
            "   - **Pros:** Simple to implement and ensures no missing data remains.\n",
            "   - **Cons:** Can result in significant loss of data, especially if missing values are prevalent.\n",
            "\n",
            "2. **Imputation with Mean/Median/Mode:**\n",
            "   - **Pros:** Preserves data size, simple to implement, and works well with numerical data.\n",
            "   - **Cons:** Can introduce bias, particularly if the missing data is not randomly distributed.\n",
            "\n",
            "3. **Using Algorithms that Support Missing Values:**\n",
            "   - **Pros:** Some machine learning algorithms (e.g., decision trees) can handle missing values natively, avoiding the need for imputation.\n",
            "   - **Cons:** Limited to specific algorithms and may not be applicable for all types of data.\n",
            "\n",
            "### Question 4: Feature Engineering\n",
            "**Question Statement:**\n",
            "You have a dataset containing the timestamp of transactions. Explain how you can create new features from the timestamp to improve the predictive power of your model.\n",
            "\n",
            "**Correct Answer:**\n",
            "From the timestamp, you can extract several useful features:\n",
            "1. **Day of the week:** Indicates if the transaction was made on a weekday or weekend, which can capture weekly patterns.\n",
            "2. **Hour of the day:** Helps in understanding the time-of-day effect on transactions.\n",
            "3. **Month of the year:** Captures seasonal effects.\n",
            "4. **Holiday indicator:** Indicates whether the transaction occurred on a holiday, which can affect transaction volume.\n",
            "5. **Elapsed time since previous transaction:** Provides temporal spacing between transactions, useful for time-series analysis.\n",
            "\n",
            "### Question 5: Model Development\n",
            "**Question Statement:**\n",
            "Describe the steps involved in developing a machine learning model from scratch, starting from data collection to model deployment.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Collection:** Gather data relevant to the problem.\n",
            "2. **Data Cleaning:** Handle missing values, remove duplicates, and correct errors.\n",
            "3. **Exploratory Data Analysis (EDA):** Understand the data distribution, relationships, and patterns.\n",
            "4. **Feature Engineering:** Create new features, normalize/scale data, and select important features.\n",
            "5. **Model Selection:** Choose appropriate algorithms based on the problem type and data characteristics.\n",
            "6. **Model Training:** Split data into training and validation sets, and train the model.\n",
            "7. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, F1-score, etc.\n",
            "8. **Hyperparameter Tuning:** Optimize model performance through techniques like grid search or random search.\n",
            "9. **Model Validation:** Validate the model on a separate test set to ensure it generalizes well.\n",
            "10. **Model Deployment:** Deploy the model in a production environment and monitor its performance.\n",
            "\n",
            "### Question 6: Data Analysis\n",
            "**Question Statement:**\n",
            "You have a dataset containing customer reviews for a product. Explain how you would perform sentiment analysis on this dataset and the steps involved.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Preprocessing:** Clean the text data by removing noise (e.g., HTML tags, special characters), converting text to lowercase, and tokenizing.\n",
            "2. **Text Normalization:** Apply stemming or lemmatization to reduce words to their base forms.\n",
            "3. **Feature Extraction:** Convert text to numerical features using techniques like TF-IDF, word embeddings, or bag-of-words.\n",
            "4. **Model Selection:** Choose a sentiment analysis model (e.g., Naive Bayes, LSTM, Transformer-based models).\n",
            "5. **Model Training:** Train the model using labeled sentiment data (positive/negative/neutral).\n",
            "6. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, and F1-score.\n",
            "7. **Prediction:** Apply the trained model to predict the sentiment of customer reviews in the dataset.\n",
            "\n",
            "### Question 7: Programming\n",
            "**Question Statement:**\n",
            "Write a Python function to calculate the F1-score given the precision and recall values. Explain the significance of the F1-score in model evaluation.\n",
            "\n",
            "**Correct Answer:**\n",
            "```python\n",
            "def f1_score(precision, recall):\n",
            "    if precision + recall == 0:\n",
            "        return 0\n",
            "    return 2 * (precision * recall) / (precision + recall)\n",
            "\n",
            "# Example usage:\n",
            "precision = 0.75\n",
            "recall = 0.80\n",
            "print(f1_score(precision, recall))  # Output: 0.774\n",
            "```\n",
            "\n",
            "**Significance:**\n",
            "The F1-score is the harmonic mean of precision and recall. It is significant because it provides a single metric that balances both precision and recall, especially useful in cases where there is an uneven class distribution or when both false positives and false negatives need to be minimized.\n",
            "\n",
            "### Question 8: Time Series Analysis\n",
            "**Question Statement:**\n",
            "Explain the concept of stationarity in time series analysis. How can you test for stationarity, and why is it important for time series forecasting?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Concept:** A time series is stationary if its statistical properties (mean, variance, autocorrelation) are constant over time. Stationarity is crucial because many time series forecasting methods, like ARIMA, assume the series is stationary.\n",
            "\n",
            "**Testing for Stationarity:**\n",
            "1. **Visual Inspection:** Plot the time series and look for trends, seasonality, and constant mean/variance.\n",
            "2. **Statistical Tests:** Perform tests like the Augmented Dickey-Fuller (ADF) test or the KPSS test.\n",
            "\n",
            "**Importance:** Stationarity ensures that the model parameters remain consistent over time, leading to more reliable and accurate forecasts. Non-stationary data can result in misleading model parameters and poor forecasting performance.\n",
            "\n",
            "### Question 9: Dimensionality Reduction\n",
            "**Question Statement:**\n",
            "Describe Principal Component Analysis (PCA) and its application in reducing the dimensionality of a dataset. What are the potential benefits and drawbacks of using PCA?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Description:** PCA is a dimensionality reduction technique that transforms a dataset into a set of orthogonal components, ordered by the amount of variance they explain in the data. The first few principal components capture the most significant variance, allowing for reduction in dimensionality while retaining most of the information.\n",
            "\n",
            "**Application:**\n",
            "1. Standardize the data.\n",
            "2. Compute the covariance matrix.\n",
            "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
            "4. Sort the eigenvectors by eigenvalues in descending order.\n",
            "5. Select the top k eigenvectors to form the new feature space.\n",
            "\n",
            "**Benefits:**\n",
            "- Reduces computational complexity.\n",
            "- Mitigates the curse of dimensionality.\n",
            "- Removes noise and redundant features.\n",
            "\n",
            "**Drawbacks:**\n",
            "- PCA is sensitive to scaling.\n",
            "- It may lose interpretability of the transformed features.\n",
            "- Not suitable for non-linear relationships.\n",
            "\n",
            "### Question 10: Model Evaluation\n",
            "**Question Statement:**\n",
            "You have built a classification model and obtained the following confusion matrix:\n",
            "\n",
            "|            | Predicted Positive | Predicted Negative |\n",
            "|------------|---------------------|---------------------|\n",
            "| Actual Positive | 50                  | 10                  |\n",
            "| Actual Negative | 5                   | 35                  |\n",
            "\n",
            "Calculate the following metrics: accuracy, precision, recall, and F1-score.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Accuracy:**\n",
            "   \\[\n",
            "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{50 + 35}{50 + 35 + 5 + 10} = \\frac{85}{100} = 0.85\n",
            "   \\]\n",
            "\n",
            "2. **Precision:**\n",
            "   \\[\n",
            "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{50}{50 + 5} = \\frac{50}{55} \\approx 0.909\n",
            "   \\]\n",
            "\n",
            "3. **Recall:**\n",
            "   \\[\n",
            "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{50}{50 + 10} = \\frac{50}{60} \\approx 0.833\n",
            "   \\]\n",
            "\n",
            "4. **F1-score:**\n",
            "   \\[\n",
            "   \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.909 \\times 0.833}{0.909 + 0.833} \\approx 0.87\n",
            "   \\]\n",
            "\n",
            "---\n",
            "\n",
            "These questions and answers assess a wide range of skills and concepts in data science, including statistical analysis, machine learning, data cleaning, feature engineering, and programming, ensuring a comprehensive evaluation of the students' understanding and abilities.\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-01 08:14:46][DEBUG]: == Working Agent:  Question Reviewer\u001b[00m\n",
            "\u001b[1m\u001b[95m [2024-09-01 08:14:46][INFO]: == Starting Task: Review the questions generated by the question_creator agent.\n",
            "Assess the question's quality, relevance to the IT industry standards, difficulty level, and clarity.\n",
            "Provide constructive feedback and suggestions for improvement, if necessary.\n",
            "Ensure that the question is well-structured, unambiguous, and aligns with the IT Industry standards.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
            "Final Answer: \n",
            "\n",
            "### Question 1: Statistics and Probability\n",
            "**Question Statement:**\n",
            "Given a dataset containing the heights (in cm) of 1000 students, you are required to calculate the 95% confidence interval for the mean height. Assume the heights follow a normal distribution with a sample mean of 165 cm and a standard deviation of 10 cm.\n",
            "\n",
            "**Correct Answer:**\n",
            "To calculate the 95% confidence interval for the mean height:\n",
            "1. Identify the sample mean (\\(\\bar{x}\\)) = 165 cm.\n",
            "2. Identify the standard deviation (\\(\\sigma\\)) = 10 cm.\n",
            "3. Determine the z-value for a 95% confidence interval, which is 1.96.\n",
            "4. Use the formula for the confidence interval: \\(\\bar{x} \\pm z \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\).\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 1.96 \\left(\\frac{10}{\\sqrt{1000}}\\right)\n",
            "   \\]\n",
            "\n",
            "5. Calculate the margin of error:\n",
            "\n",
            "   \\[\n",
            "   1.96 \\left(\\frac{10}{31.62}\\right) \\approx 1.96 \\times 0.3162 \\approx 0.62\n",
            "   \\]\n",
            "\n",
            "6. The confidence interval is:\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 0.62 \\rightarrow [164.38, 165.62]\n",
            "   \\]\n",
            "\n",
            "### Question 2: Machine Learning\n",
            "**Question Statement:**\n",
            "Explain the bias-variance tradeoff and how it impacts the performance of machine learning models. Provide a real-world example to illustrate your explanation.\n",
            "\n",
            "**Correct Answer:**\n",
            "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between the error due to bias and the error due to variance. \n",
            "\n",
            "- **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).\n",
            "- **Variance** refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).\n",
            "\n",
            "**Real-world Example:**\n",
            "Consider a model predicting house prices. A linear regression model may have high bias because it cannot capture non-linear relationships in the data, resulting in underfitting. On the other hand, a complex model like a high-degree polynomial regression may have high variance, capturing noise and leading to overfitting. The goal is to find a balance where both bias and variance are minimized, often through techniques like cross-validation, regularization, and choosing appropriate model complexity.\n",
            "\n",
            "### Question 3: Data Cleaning\n",
            "**Question Statement:**\n",
            "You are given a dataset with missing values in several columns. Describe at least three techniques you can use to handle missing data and discuss the pros and cons of each method.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Removing Rows with Missing Values:**\n",
            "   - **Pros:** Simple to implement and ensures no missing data remains.\n",
            "   - **Cons:** Can result in significant loss of data, especially if missing values are prevalent.\n",
            "\n",
            "2. **Imputation with Mean/Median/Mode:**\n",
            "   - **Pros:** Preserves data size, simple to implement, and works well with numerical data.\n",
            "   - **Cons:** Can introduce bias, particularly if the missing data is not randomly distributed.\n",
            "\n",
            "3. **Using Algorithms that Support Missing Values:**\n",
            "   - **Pros:** Some machine learning algorithms (e.g., decision trees) can handle missing values natively, avoiding the need for imputation.\n",
            "   - **Cons:** Limited to specific algorithms and may not be applicable for all types of data.\n",
            "\n",
            "### Question 4: Feature Engineering\n",
            "**Question Statement:**\n",
            "You have a dataset containing the timestamp of transactions. Explain how you can create new features from the timestamp to improve the predictive power of your model.\n",
            "\n",
            "**Correct Answer:**\n",
            "From the timestamp, you can extract several useful features:\n",
            "1. **Day of the week:** Indicates if the transaction was made on a weekday or weekend, which can capture weekly patterns.\n",
            "2. **Hour of the day:** Helps in understanding the time-of-day effect on transactions.\n",
            "3. **Month of the year:** Captures seasonal effects.\n",
            "4. **Holiday indicator:** Indicates whether the transaction occurred on a holiday, which can affect transaction volume.\n",
            "5. **Elapsed time since previous transaction:** Provides temporal spacing between transactions, useful for time-series analysis.\n",
            "\n",
            "### Question 5: Model Development\n",
            "**Question Statement:**\n",
            "Describe the steps involved in developing a machine learning model from scratch, starting from data collection to model deployment.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Collection:** Gather data relevant to the problem.\n",
            "2. **Data Cleaning:** Handle missing values, remove duplicates, and correct errors.\n",
            "3. **Exploratory Data Analysis (EDA):** Understand the data distribution, relationships, and patterns.\n",
            "4. **Feature Engineering:** Create new features, normalize/scale data, and select important features.\n",
            "5. **Model Selection:** Choose appropriate algorithms based on the problem type and data characteristics.\n",
            "6. **Model Training:** Split data into training and validation sets, and train the model.\n",
            "7. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, F1-score, etc.\n",
            "8. **Hyperparameter Tuning:** Optimize model performance through techniques like grid search or random search.\n",
            "9. **Model Validation:** Validate the model on a separate test set to ensure it generalizes well.\n",
            "10. **Model Deployment:** Deploy the model in a production environment and monitor its performance.\n",
            "\n",
            "### Question 6: Data Analysis\n",
            "**Question Statement:**\n",
            "You have a dataset containing customer reviews for a product. Explain how you would perform sentiment analysis on this dataset and the steps involved.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Preprocessing:** Clean the text data by removing noise (e.g., HTML tags, special characters), converting text to lowercase, and tokenizing.\n",
            "2. **Text Normalization:** Apply stemming or lemmatization to reduce words to their base forms.\n",
            "3. **Feature Extraction:** Convert text to numerical features using techniques like TF-IDF, word embeddings, or bag-of-words.\n",
            "4. **Model Selection:** Choose a sentiment analysis model (e.g., Naive Bayes, LSTM, Transformer-based models).\n",
            "5. **Model Training:** Train the model using labeled sentiment data (positive/negative/neutral).\n",
            "6. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, and F1-score.\n",
            "7. **Prediction:** Apply the trained model to predict the sentiment of customer reviews in the dataset.\n",
            "\n",
            "### Question 7: Programming\n",
            "**Question Statement:**\n",
            "Write a Python function to calculate the F1-score given the precision and recall values. Explain the significance of the F1-score in model evaluation.\n",
            "\n",
            "**Correct Answer:**\n",
            "```python\n",
            "def f1_score(precision, recall):\n",
            "    if precision + recall == 0:\n",
            "        return 0\n",
            "    return 2 * (precision * recall) / (precision + recall)\n",
            "\n",
            "# Example usage:\n",
            "precision = 0.75\n",
            "recall = 0.80\n",
            "print(f1_score(precision, recall))  # Output: 0.774\n",
            "```\n",
            "\n",
            "**Significance:**\n",
            "The F1-score is the harmonic mean of precision and recall. It is significant because it provides a single metric that balances both precision and recall, especially useful in cases where there is an uneven class distribution or when both false positives and false negatives need to be minimized.\n",
            "\n",
            "### Question 8: Time Series Analysis\n",
            "**Question Statement:**\n",
            "Explain the concept of stationarity in time series analysis. How can you test for stationarity, and why is it important for time series forecasting?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Concept:** A time series is stationary if its statistical properties (mean, variance, autocorrelation) are constant over time. Stationarity is crucial because many time series forecasting methods, like ARIMA, assume the series is stationary.\n",
            "\n",
            "**Testing for Stationarity:**\n",
            "1. **Visual Inspection:** Plot the time series and look for trends, seasonality, and constant mean/variance.\n",
            "2. **Statistical Tests:** Perform tests like the Augmented Dickey-Fuller (ADF) test or the KPSS test.\n",
            "\n",
            "**Importance:** Stationarity ensures that the model parameters remain consistent over time, leading to more reliable and accurate forecasts. Non-stationary data can result in misleading model parameters and poor forecasting performance.\n",
            "\n",
            "### Question 9: Dimensionality Reduction\n",
            "**Question Statement:**\n",
            "Describe Principal Component Analysis (PCA) and its application in reducing the dimensionality of a dataset. What are the potential benefits and drawbacks of using PCA?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Description:** PCA is a dimensionality reduction technique that transforms a dataset into a set of orthogonal components, ordered by the amount of variance they explain in the data. The first few principal components capture the most significant variance, allowing for reduction in dimensionality while retaining most of the information.\n",
            "\n",
            "**Application:**\n",
            "1. Standardize the data.\n",
            "2. Compute the covariance matrix.\n",
            "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
            "4. Sort the eigenvectors by eigenvalues in descending order.\n",
            "5. Select the top k eigenvectors to form the new feature space.\n",
            "\n",
            "**Benefits:**\n",
            "- Reduces computational complexity.\n",
            "- Mitigates the curse of dimensionality.\n",
            "- Removes noise and redundant features.\n",
            "\n",
            "**Drawbacks:**\n",
            "- PCA is sensitive to scaling.\n",
            "- It may lose interpretability of the transformed features.\n",
            "- Not suitable for non-linear relationships.\n",
            "\n",
            "### Question 10: Model Evaluation\n",
            "**Question Statement:**\n",
            "You have built a classification model and obtained the following confusion matrix:\n",
            "\n",
            "|            | Predicted Positive | Predicted Negative |\n",
            "|------------|---------------------|---------------------|\n",
            "| Actual Positive | 50                  | 10                  |\n",
            "| Actual Negative | 5                   | 35                  |\n",
            "\n",
            "Calculate the following metrics: accuracy, precision, recall, and F1-score.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Accuracy:**\n",
            "   \\[\n",
            "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{50 + 35}{50 + 35 + 5 + 10} = \\frac{85}{100} = 0.85\n",
            "   \\]\n",
            "\n",
            "2. **Precision:**\n",
            "   \\[\n",
            "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{50}{50 + 5} = \\frac{50}{55} \\approx 0.909\n",
            "   \\]\n",
            "\n",
            "3. **Recall:**\n",
            "   \\[\n",
            "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{50}{50 + 10} = \\frac{50}{60} \\approx 0.833\n",
            "   \\]\n",
            "\n",
            "4. **F1-score:**\n",
            "   \\[\n",
            "   \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.909 \\times 0.833}{0.909 + 0.833} \\approx 0.87\n",
            "   \\]\n",
            "\n",
            "---\n",
            "\n",
            "These questions and answers assess a wide range of skills and concepts in data science, including statistical analysis, machine learning, data cleaning, feature engineering, and programming, ensuring a comprehensive evaluation of the students' understanding and abilities.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[1m\u001b[92m [2024-09-01 08:15:49][DEBUG]: == [ Question Reviewer] Task output: ### Question 1: Statistics and Probability\n",
            "**Question Statement:**\n",
            "Given a dataset containing the heights (in cm) of 1000 students, you are required to calculate the 95% confidence interval for the mean height. Assume the heights follow a normal distribution with a sample mean of 165 cm and a standard deviation of 10 cm.\n",
            "\n",
            "**Correct Answer:**\n",
            "To calculate the 95% confidence interval for the mean height:\n",
            "1. Identify the sample mean (\\(\\bar{x}\\)) = 165 cm.\n",
            "2. Identify the standard deviation (\\(\\sigma\\)) = 10 cm.\n",
            "3. Determine the z-value for a 95% confidence interval, which is 1.96.\n",
            "4. Use the formula for the confidence interval: \\(\\bar{x} \\pm z \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\).\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 1.96 \\left(\\frac{10}{\\sqrt{1000}}\\right)\n",
            "   \\]\n",
            "\n",
            "5. Calculate the margin of error:\n",
            "\n",
            "   \\[\n",
            "   1.96 \\left(\\frac{10}{31.62}\\right) \\approx 1.96 \\times 0.3162 \\approx 0.62\n",
            "   \\]\n",
            "\n",
            "6. The confidence interval is:\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 0.62 \\rightarrow [164.38, 165.62]\n",
            "   \\]\n",
            "\n",
            "### Question 2: Machine Learning\n",
            "**Question Statement:**\n",
            "Explain the bias-variance tradeoff and how it impacts the performance of machine learning models. Provide a real-world example to illustrate your explanation.\n",
            "\n",
            "**Correct Answer:**\n",
            "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between the error due to bias and the error due to variance. \n",
            "\n",
            "- **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).\n",
            "- **Variance** refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).\n",
            "\n",
            "**Real-world Example:**\n",
            "Consider a model predicting house prices. A linear regression model may have high bias because it cannot capture non-linear relationships in the data, resulting in underfitting. On the other hand, a complex model like a high-degree polynomial regression may have high variance, capturing noise and leading to overfitting. The goal is to find a balance where both bias and variance are minimized, often through techniques like cross-validation, regularization, and choosing appropriate model complexity.\n",
            "\n",
            "### Question 3: Data Cleaning\n",
            "**Question Statement:**\n",
            "You are given a dataset with missing values in several columns. Describe at least three techniques you can use to handle missing data and discuss the pros and cons of each method.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Removing Rows with Missing Values:**\n",
            "   - **Pros:** Simple to implement and ensures no missing data remains.\n",
            "   - **Cons:** Can result in significant loss of data, especially if missing values are prevalent.\n",
            "\n",
            "2. **Imputation with Mean/Median/Mode:**\n",
            "   - **Pros:** Preserves data size, simple to implement, and works well with numerical data.\n",
            "   - **Cons:** Can introduce bias, particularly if the missing data is not randomly distributed.\n",
            "\n",
            "3. **Using Algorithms that Support Missing Values:**\n",
            "   - **Pros:** Some machine learning algorithms (e.g., decision trees) can handle missing values natively, avoiding the need for imputation.\n",
            "   - **Cons:** Limited to specific algorithms and may not be applicable for all types of data.\n",
            "\n",
            "### Question 4: Feature Engineering\n",
            "**Question Statement:**\n",
            "You have a dataset containing the timestamp of transactions. Explain how you can create new features from the timestamp to improve the predictive power of your model.\n",
            "\n",
            "**Correct Answer:**\n",
            "From the timestamp, you can extract several useful features:\n",
            "1. **Day of the week:** Indicates if the transaction was made on a weekday or weekend, which can capture weekly patterns.\n",
            "2. **Hour of the day:** Helps in understanding the time-of-day effect on transactions.\n",
            "3. **Month of the year:** Captures seasonal effects.\n",
            "4. **Holiday indicator:** Indicates whether the transaction occurred on a holiday, which can affect transaction volume.\n",
            "5. **Elapsed time since previous transaction:** Provides temporal spacing between transactions, useful for time-series analysis.\n",
            "\n",
            "### Question 5: Model Development\n",
            "**Question Statement:**\n",
            "Describe the steps involved in developing a machine learning model from scratch, starting from data collection to model deployment.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Collection:** Gather data relevant to the problem.\n",
            "2. **Data Cleaning:** Handle missing values, remove duplicates, and correct errors.\n",
            "3. **Exploratory Data Analysis (EDA):** Understand the data distribution, relationships, and patterns.\n",
            "4. **Feature Engineering:** Create new features, normalize/scale data, and select important features.\n",
            "5. **Model Selection:** Choose appropriate algorithms based on the problem type and data characteristics.\n",
            "6. **Model Training:** Split data into training and validation sets, and train the model.\n",
            "7. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, F1-score, etc.\n",
            "8. **Hyperparameter Tuning:** Optimize model performance through techniques like grid search or random search.\n",
            "9. **Model Validation:** Validate the model on a separate test set to ensure it generalizes well.\n",
            "10. **Model Deployment:** Deploy the model in a production environment and monitor its performance.\n",
            "\n",
            "### Question 6: Data Analysis\n",
            "**Question Statement:**\n",
            "You have a dataset containing customer reviews for a product. Explain how you would perform sentiment analysis on this dataset and the steps involved.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Preprocessing:** Clean the text data by removing noise (e.g., HTML tags, special characters), converting text to lowercase, and tokenizing.\n",
            "2. **Text Normalization:** Apply stemming or lemmatization to reduce words to their base forms.\n",
            "3. **Feature Extraction:** Convert text to numerical features using techniques like TF-IDF, word embeddings, or bag-of-words.\n",
            "4. **Model Selection:** Choose a sentiment analysis model (e.g., Naive Bayes, LSTM, Transformer-based models).\n",
            "5. **Model Training:** Train the model using labeled sentiment data (positive/negative/neutral).\n",
            "6. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, and F1-score.\n",
            "7. **Prediction:** Apply the trained model to predict the sentiment of customer reviews in the dataset.\n",
            "\n",
            "### Question 7: Programming\n",
            "**Question Statement:**\n",
            "Write a Python function to calculate the F1-score given the precision and recall values. Explain the significance of the F1-score in model evaluation.\n",
            "\n",
            "**Correct Answer:**\n",
            "```python\n",
            "def f1_score(precision, recall):\n",
            "    if precision + recall == 0:\n",
            "        return 0\n",
            "    return 2 * (precision * recall) / (precision + recall)\n",
            "\n",
            "# Example usage:\n",
            "precision = 0.75\n",
            "recall = 0.80\n",
            "print(f1_score(precision, recall))  # Output: 0.774\n",
            "```\n",
            "\n",
            "**Significance:**\n",
            "The F1-score is the harmonic mean of precision and recall. It is significant because it provides a single metric that balances both precision and recall, especially useful in cases where there is an uneven class distribution or when both false positives and false negatives need to be minimized.\n",
            "\n",
            "### Question 8: Time Series Analysis\n",
            "**Question Statement:**\n",
            "Explain the concept of stationarity in time series analysis. How can you test for stationarity, and why is it important for time series forecasting?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Concept:** A time series is stationary if its statistical properties (mean, variance, autocorrelation) are constant over time. Stationarity is crucial because many time series forecasting methods, like ARIMA, assume the series is stationary.\n",
            "\n",
            "**Testing for Stationarity:**\n",
            "1. **Visual Inspection:** Plot the time series and look for trends, seasonality, and constant mean/variance.\n",
            "2. **Statistical Tests:** Perform tests like the Augmented Dickey-Fuller (ADF) test or the KPSS test.\n",
            "\n",
            "**Importance:** Stationarity ensures that the model parameters remain consistent over time, leading to more reliable and accurate forecasts. Non-stationary data can result in misleading model parameters and poor forecasting performance.\n",
            "\n",
            "### Question 9: Dimensionality Reduction\n",
            "**Question Statement:**\n",
            "Describe Principal Component Analysis (PCA) and its application in reducing the dimensionality of a dataset. What are the potential benefits and drawbacks of using PCA?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Description:** PCA is a dimensionality reduction technique that transforms a dataset into a set of orthogonal components, ordered by the amount of variance they explain in the data. The first few principal components capture the most significant variance, allowing for reduction in dimensionality while retaining most of the information.\n",
            "\n",
            "**Application:**\n",
            "1. Standardize the data.\n",
            "2. Compute the covariance matrix.\n",
            "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
            "4. Sort the eigenvectors by eigenvalues in descending order.\n",
            "5. Select the top k eigenvectors to form the new feature space.\n",
            "\n",
            "**Benefits:**\n",
            "- Reduces computational complexity.\n",
            "- Mitigates the curse of dimensionality.\n",
            "- Removes noise and redundant features.\n",
            "\n",
            "**Drawbacks:**\n",
            "- PCA is sensitive to scaling.\n",
            "- It may lose interpretability of the transformed features.\n",
            "- Not suitable for non-linear relationships.\n",
            "\n",
            "### Question 10: Model Evaluation\n",
            "**Question Statement:**\n",
            "You have built a classification model and obtained the following confusion matrix:\n",
            "\n",
            "|            | Predicted Positive | Predicted Negative |\n",
            "|------------|---------------------|---------------------|\n",
            "| Actual Positive | 50                  | 10                  |\n",
            "| Actual Negative | 5                   | 35                  |\n",
            "\n",
            "Calculate the following metrics: accuracy, precision, recall, and F1-score.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Accuracy:**\n",
            "   \\[\n",
            "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{50 + 35}{50 + 35 + 5 + 10} = \\frac{85}{100} = 0.85\n",
            "   \\]\n",
            "\n",
            "2. **Precision:**\n",
            "   \\[\n",
            "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{50}{50 + 5} = \\frac{50}{55} \\approx 0.909\n",
            "   \\]\n",
            "\n",
            "3. **Recall:**\n",
            "   \\[\n",
            "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{50}{50 + 10} = \\frac{50}{60} \\approx 0.833\n",
            "   \\]\n",
            "\n",
            "4. **F1-score:**\n",
            "   \\[\n",
            "   \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.909 \\times 0.833}{0.909 + 0.833} \\approx 0.87\n",
            "   \\]\n",
            "\n",
            "---\n",
            "\n",
            "These questions and answers assess a wide range of skills and concepts in data science, including statistical analysis, machine learning, data cleaning, feature engineering, and programming, ensuring a comprehensive evaluation of the students' understanding and abilities.\n",
            "\n",
            "\u001b[00m\n",
            "######################\n",
            "### Question 1: Statistics and Probability\n",
            "**Question Statement:**\n",
            "Given a dataset containing the heights (in cm) of 1000 students, you are required to calculate the 95% confidence interval for the mean height. Assume the heights follow a normal distribution with a sample mean of 165 cm and a standard deviation of 10 cm.\n",
            "\n",
            "**Correct Answer:**\n",
            "To calculate the 95% confidence interval for the mean height:\n",
            "1. Identify the sample mean (\\(\\bar{x}\\)) = 165 cm.\n",
            "2. Identify the standard deviation (\\(\\sigma\\)) = 10 cm.\n",
            "3. Determine the z-value for a 95% confidence interval, which is 1.96.\n",
            "4. Use the formula for the confidence interval: \\(\\bar{x} \\pm z \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\).\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 1.96 \\left(\\frac{10}{\\sqrt{1000}}\\right)\n",
            "   \\]\n",
            "\n",
            "5. Calculate the margin of error:\n",
            "\n",
            "   \\[\n",
            "   1.96 \\left(\\frac{10}{31.62}\\right) \\approx 1.96 \\times 0.3162 \\approx 0.62\n",
            "   \\]\n",
            "\n",
            "6. The confidence interval is:\n",
            "\n",
            "   \\[\n",
            "   165 \\pm 0.62 \\rightarrow [164.38, 165.62]\n",
            "   \\]\n",
            "\n",
            "### Question 2: Machine Learning\n",
            "**Question Statement:**\n",
            "Explain the bias-variance tradeoff and how it impacts the performance of machine learning models. Provide a real-world example to illustrate your explanation.\n",
            "\n",
            "**Correct Answer:**\n",
            "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between the error due to bias and the error due to variance. \n",
            "\n",
            "- **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).\n",
            "- **Variance** refers to the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data rather than the intended outputs (overfitting).\n",
            "\n",
            "**Real-world Example:**\n",
            "Consider a model predicting house prices. A linear regression model may have high bias because it cannot capture non-linear relationships in the data, resulting in underfitting. On the other hand, a complex model like a high-degree polynomial regression may have high variance, capturing noise and leading to overfitting. The goal is to find a balance where both bias and variance are minimized, often through techniques like cross-validation, regularization, and choosing appropriate model complexity.\n",
            "\n",
            "### Question 3: Data Cleaning\n",
            "**Question Statement:**\n",
            "You are given a dataset with missing values in several columns. Describe at least three techniques you can use to handle missing data and discuss the pros and cons of each method.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Removing Rows with Missing Values:**\n",
            "   - **Pros:** Simple to implement and ensures no missing data remains.\n",
            "   - **Cons:** Can result in significant loss of data, especially if missing values are prevalent.\n",
            "\n",
            "2. **Imputation with Mean/Median/Mode:**\n",
            "   - **Pros:** Preserves data size, simple to implement, and works well with numerical data.\n",
            "   - **Cons:** Can introduce bias, particularly if the missing data is not randomly distributed.\n",
            "\n",
            "3. **Using Algorithms that Support Missing Values:**\n",
            "   - **Pros:** Some machine learning algorithms (e.g., decision trees) can handle missing values natively, avoiding the need for imputation.\n",
            "   - **Cons:** Limited to specific algorithms and may not be applicable for all types of data.\n",
            "\n",
            "### Question 4: Feature Engineering\n",
            "**Question Statement:**\n",
            "You have a dataset containing the timestamp of transactions. Explain how you can create new features from the timestamp to improve the predictive power of your model.\n",
            "\n",
            "**Correct Answer:**\n",
            "From the timestamp, you can extract several useful features:\n",
            "1. **Day of the week:** Indicates if the transaction was made on a weekday or weekend, which can capture weekly patterns.\n",
            "2. **Hour of the day:** Helps in understanding the time-of-day effect on transactions.\n",
            "3. **Month of the year:** Captures seasonal effects.\n",
            "4. **Holiday indicator:** Indicates whether the transaction occurred on a holiday, which can affect transaction volume.\n",
            "5. **Elapsed time since previous transaction:** Provides temporal spacing between transactions, useful for time-series analysis.\n",
            "\n",
            "### Question 5: Model Development\n",
            "**Question Statement:**\n",
            "Describe the steps involved in developing a machine learning model from scratch, starting from data collection to model deployment.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Collection:** Gather data relevant to the problem.\n",
            "2. **Data Cleaning:** Handle missing values, remove duplicates, and correct errors.\n",
            "3. **Exploratory Data Analysis (EDA):** Understand the data distribution, relationships, and patterns.\n",
            "4. **Feature Engineering:** Create new features, normalize/scale data, and select important features.\n",
            "5. **Model Selection:** Choose appropriate algorithms based on the problem type and data characteristics.\n",
            "6. **Model Training:** Split data into training and validation sets, and train the model.\n",
            "7. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, F1-score, etc.\n",
            "8. **Hyperparameter Tuning:** Optimize model performance through techniques like grid search or random search.\n",
            "9. **Model Validation:** Validate the model on a separate test set to ensure it generalizes well.\n",
            "10. **Model Deployment:** Deploy the model in a production environment and monitor its performance.\n",
            "\n",
            "### Question 6: Data Analysis\n",
            "**Question Statement:**\n",
            "You have a dataset containing customer reviews for a product. Explain how you would perform sentiment analysis on this dataset and the steps involved.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Data Preprocessing:** Clean the text data by removing noise (e.g., HTML tags, special characters), converting text to lowercase, and tokenizing.\n",
            "2. **Text Normalization:** Apply stemming or lemmatization to reduce words to their base forms.\n",
            "3. **Feature Extraction:** Convert text to numerical features using techniques like TF-IDF, word embeddings, or bag-of-words.\n",
            "4. **Model Selection:** Choose a sentiment analysis model (e.g., Naive Bayes, LSTM, Transformer-based models).\n",
            "5. **Model Training:** Train the model using labeled sentiment data (positive/negative/neutral).\n",
            "6. **Model Evaluation:** Evaluate the model using metrics like accuracy, precision, recall, and F1-score.\n",
            "7. **Prediction:** Apply the trained model to predict the sentiment of customer reviews in the dataset.\n",
            "\n",
            "### Question 7: Programming\n",
            "**Question Statement:**\n",
            "Write a Python function to calculate the F1-score given the precision and recall values. Explain the significance of the F1-score in model evaluation.\n",
            "\n",
            "**Correct Answer:**\n",
            "```python\n",
            "def f1_score(precision, recall):\n",
            "    if precision + recall == 0:\n",
            "        return 0\n",
            "    return 2 * (precision * recall) / (precision + recall)\n",
            "\n",
            "# Example usage:\n",
            "precision = 0.75\n",
            "recall = 0.80\n",
            "print(f1_score(precision, recall))  # Output: 0.774\n",
            "```\n",
            "\n",
            "**Significance:**\n",
            "The F1-score is the harmonic mean of precision and recall. It is significant because it provides a single metric that balances both precision and recall, especially useful in cases where there is an uneven class distribution or when both false positives and false negatives need to be minimized.\n",
            "\n",
            "### Question 8: Time Series Analysis\n",
            "**Question Statement:**\n",
            "Explain the concept of stationarity in time series analysis. How can you test for stationarity, and why is it important for time series forecasting?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Concept:** A time series is stationary if its statistical properties (mean, variance, autocorrelation) are constant over time. Stationarity is crucial because many time series forecasting methods, like ARIMA, assume the series is stationary.\n",
            "\n",
            "**Testing for Stationarity:**\n",
            "1. **Visual Inspection:** Plot the time series and look for trends, seasonality, and constant mean/variance.\n",
            "2. **Statistical Tests:** Perform tests like the Augmented Dickey-Fuller (ADF) test or the KPSS test.\n",
            "\n",
            "**Importance:** Stationarity ensures that the model parameters remain consistent over time, leading to more reliable and accurate forecasts. Non-stationary data can result in misleading model parameters and poor forecasting performance.\n",
            "\n",
            "### Question 9: Dimensionality Reduction\n",
            "**Question Statement:**\n",
            "Describe Principal Component Analysis (PCA) and its application in reducing the dimensionality of a dataset. What are the potential benefits and drawbacks of using PCA?\n",
            "\n",
            "**Correct Answer:**\n",
            "**Description:** PCA is a dimensionality reduction technique that transforms a dataset into a set of orthogonal components, ordered by the amount of variance they explain in the data. The first few principal components capture the most significant variance, allowing for reduction in dimensionality while retaining most of the information.\n",
            "\n",
            "**Application:**\n",
            "1. Standardize the data.\n",
            "2. Compute the covariance matrix.\n",
            "3. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
            "4. Sort the eigenvectors by eigenvalues in descending order.\n",
            "5. Select the top k eigenvectors to form the new feature space.\n",
            "\n",
            "**Benefits:**\n",
            "- Reduces computational complexity.\n",
            "- Mitigates the curse of dimensionality.\n",
            "- Removes noise and redundant features.\n",
            "\n",
            "**Drawbacks:**\n",
            "- PCA is sensitive to scaling.\n",
            "- It may lose interpretability of the transformed features.\n",
            "- Not suitable for non-linear relationships.\n",
            "\n",
            "### Question 10: Model Evaluation\n",
            "**Question Statement:**\n",
            "You have built a classification model and obtained the following confusion matrix:\n",
            "\n",
            "|            | Predicted Positive | Predicted Negative |\n",
            "|------------|---------------------|---------------------|\n",
            "| Actual Positive | 50                  | 10                  |\n",
            "| Actual Negative | 5                   | 35                  |\n",
            "\n",
            "Calculate the following metrics: accuracy, precision, recall, and F1-score.\n",
            "\n",
            "**Correct Answer:**\n",
            "1. **Accuracy:**\n",
            "   \\[\n",
            "   \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{50 + 35}{50 + 35 + 5 + 10} = \\frac{85}{100} = 0.85\n",
            "   \\]\n",
            "\n",
            "2. **Precision:**\n",
            "   \\[\n",
            "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{50}{50 + 5} = \\frac{50}{55} \\approx 0.909\n",
            "   \\]\n",
            "\n",
            "3. **Recall:**\n",
            "   \\[\n",
            "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{50}{50 + 10} = \\frac{50}{60} \\approx 0.833\n",
            "   \\]\n",
            "\n",
            "4. **F1-score:**\n",
            "   \\[\n",
            "   \\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.909 \\times 0.833}{0.909 + 0.833} \\approx 0.87\n",
            "   \\]\n",
            "\n",
            "---\n",
            "\n",
            "These questions and answers assess a wide range of skills and concepts in data science, including statistical analysis, machine learning, data cleaning, feature engineering, and programming, ensuring a comprehensive evaluation of the students' understanding and abilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4i5HXSpPFpm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhMTSFohFpko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}